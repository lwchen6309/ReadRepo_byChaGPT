# ReadRepo_byChaGPT
A tutorial aims to guide the researcher who's not familiar with deep learning to read through th a deep learning repository by using ChatGPT, especially to traverse the core function of repo to align with the scientific paper accordingly.

We start from [Segment Anything](https://github.com/facebookresearch/segment-anything) SAM repo.

![repo_tutor](https://user-images.githubusercontent.com/42672685/235139891-3e697565-bf5f-4464-909a-d18cfbf12378.jpg)

The general structure of girhub repo is below:
- `setup.py` indicates that this repo can installed by command 
```pip install -e .```
- `scripts` consists of the demo script of the repo.
- `segment_anything` (sometimes also named as `src`) contains the core code of the repo.

Let's start from the demo script, which is scripts/amg.py as indicated in SAM repo: 
```
python scripts/amg.py --checkpoint <path/to/checkpoint> --model-type <model_type> --input <image_or_folder> --output <path/to/output>
```

Simply, paste all the code of `scripts/amg.py` into ChatGPT with :

```
Please explain the structure of this code, and show me where the segment-anything is implemented:
```paste the code of amg.py here```
```
Recieving the results from GPT would return something like this (there is no guarantee of this answer dues to its randomness of answer, try few more time if it return something else):
```
...
The script imports the following modules:

- cv2 (OpenCV) for image processing and manipulation.
- segment_anything which contains the SamAutomaticMaskGenerator class and the sam_model_registry for model loading and mask generation.
- argparse for command-line argument parsing.
- json for writing COCO-style RLE masks to a JSON file.
- os for creating directories, joining paths, and checking if paths exist.
- typing for type annotations.
- The script defines a function write_masks_to_folder() that takes a list of masks and writes them as PNG images to a specified directory. The function also generates metadata for each mask and writes it to a CSV file in the same directory.

Another function get_amg_kwargs() takes the parsed command-line arguments and returns a dictionary of arguments for the SamAutomaticMaskGenerator class.
...
```
As indicated, `SamAutomaticMaskGenerator` is the core function implement SAM to be traversed. 
We now check the top of `scripts/amg.py` to traverse where its imported.
```
import cv2  # type: ignore

from segment_anything import SamAutomaticMaskGenerator, sam_model_registry
...
```
and find that it's imported from segment_anything, which is a dir and a module defined by the `__init__.py` under `segment_anything`, whose code is shown below. 
```
# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.

# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

from .build_sam import (
    build_sam,
    build_sam_vit_h,
    build_sam_vit_l,
    build_sam_vit_b,
    sam_model_registry,
)
from .predictor import SamPredictor
from .automatic_mask_generator import SamAutomaticMaskGenerator
```
Now, we can see from this line 
```
from .automatic_mask_generator import SamAutomaticMaskGenerator
```
that the SAM is implemented at `automatic_mask_generator.py` under `segment_anything`. Namely, `segment_anything/automatic_mask_generator.py`. Checking this code, we then find
```
...

class SamAutomaticMaskGenerator:
    def __init__(
        self,
        model: Sam,
        points_per_side: Optional[int] = 32,
        points_per_batch: int = 64,
        pred_iou_thresh: float = 0.88,
        stability_score_thresh: float = 0.95,
        stability_score_offset: float = 1.0,
        box_nms_thresh: float = 0.7,
        crop_n_layers: int = 0,
        crop_nms_thresh: float = 0.7,
        crop_overlap_ratio: float = 512 / 1500,
        crop_n_points_downscale_factor: int = 1,
        point_grids: Optional[List[np.ndarray]] = None,
        min_mask_region_area: int = 0,
        output_mode: str = "binary_mask",
    ) -> None:
```
Again, paste the code of the whole class into GPT:
```
This is a Python class called `SamAutomaticMaskGenerator` that generates masks for an image using a given SAM model. The masks are generated by sampling a grid of points over the image, filtering low-quality and duplicate masks, and applying post-processing to remove small disconnected regions and holes in masks. The class provides several parameters to control the mask generation process, such as the number of points to be sampled, the threshold for filtering masks based on their quality, and the output format of the masks.

The class has three main methods:
- `__init__`: Initializes the class and sets the parameters for mask generation.
- `generate`: Generates masks for a given image and returns a list of dictionaries containing information about each mask.
- `_generate_masks`: Generates masks for a given image by iterating over image crops and calling `_process_crop` for each crop.
- `_process_crop`: Generates masks for a given image crop by sampling a grid of points, running the model on the points, filtering low-quality and duplicate masks, and compressing the masks to RLE format.
- `_process_batch`: Runs the model on a batch of points and returns the predicted masks and associated data.

The class also has a static method called `postprocess_small_regions` that removes small disconnected regions and holes in masks and reruns box NMS to remove any new duplicates. This method requires OpenCV as a dependency.
```

We are now good to check its implementation in `_generate_masks`. Again, paste the code as follows:
```
Please explain this code, and show me where segment anything is implemented and how to do it.
```paste code here```
```
Resultsing the following answer:
```
The code provided is a Python class `SamAutomaticMaskGenerator` that generates masks for an entire image using the Spatial Attention Module (SAM) model. The input to the generator is an RGB image, and the output is a list of masks in a specified format. 

The SAM model is a deep learning model that generates attention maps for images. It uses a backbone such as a convolutional neural network (CNN) or a Vision Transformer (ViT) and a decoder to output a multi-resolution mask of the image. The decoder uses spatial attention to fuse features across multiple resolutions to generate the final mask. 

The `SamAutomaticMaskGenerator` class has the following main methods:

1. `__init__()`: Initializes the generator with a SAM model and various parameters that control how masks are generated and processed.
2. `generate()`: Generates masks for an input image.
3. `_generate_masks()`: Generates masks for an input image crop.
4. `_process_crop()`: Processes a crop of an input image.
5. `_process_batch()`: Processes a batch of points within a crop of an input image.
6. `postprocess_small_regions()`: Removes small disconnected regions and holes in masks, then reruns box NMS to remove any new duplicates.

The `SamAutomaticMaskGenerator` class has the following parameters:

1. `model`: A SAM model.
2. `points_per_side`: The number of points to be sampled along one side of the image.
3. `points_per_batch`: The number of points run simultaneously by the model.
4. `pred_iou_thresh`: A filtering threshold in [0,1], using the model's predicted mask quality.
5. `stability_score_thresh`: A filtering threshold in [0,1], using the stability of the mask under changes to the cutoff used to binarize the model's mask predictions.
6. `stability_score_offset`: The amount to shift the cutoff when calculating the stability score.
7. `box_nms_thresh`: The box IoU cutoff used by non-maximal suppression to filter duplicate masks.
8. `crop_n_layers`: If >0, mask prediction will be run again on crops of the image.
9. `crop_nms_thresh`: The box IoU cutoff used by non-maximal suppression to filter duplicate masks between different crops.
10. `crop_overlap_ratio`: Sets the degree to which crops overlap.
11. `crop_n_points_downscale_factor`: The number of points-per-side sampled in layer n is scaled down by `crop_n_points_downscale_factor**n`.
12. `point_grids`: A list over explicit grids of points used for sampling, normalized to [0,1].
13. `min_mask_region_area`: If >0, postprocessing will be applied to remove disconnected regions and holes in masks with area smaller than `min_mask_region_area`.
14. `output_mode`: The form masks are returned in. Can be 'binary_mask', 'uncompressed_rle', or 'coco_rle'.

The `SamAutomaticMaskGenerator` class generates masks by sampling points over an image, processing each sample point batch in parallel, thresholding masks to binary, removing small disconnected regions and holes, filtering masks by predicted IoU and stability, and encoding the final masks. The class uses `batched_nms()` function from torchvision.ops.boxes to perform non-maximum suppression. 

Regarding the implementation of segmentation, the generator generates masks through the following steps:

1. An input image is divided into crops to reduce the computation complexity.
2. For each crop, a set of point samples are generated.
3. For each set of points, the model predicts a set of binary masks with different resolutions.
...
```

You can then further ask GPT about the detail of this core function, and compare the algo in paper if needed.
